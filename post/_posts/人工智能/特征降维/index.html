<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>特征降维 - Zput's blog</title>
<meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=Cache-Control content="no-transform"><meta http-equiv=Cache-Control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="zput"><meta name=description content="降维是指在某些限定条件下，降低随机变量（特征）个数，得到一组“不相关”主变量的过程。 降低随机变量的个数 正是因为在进行训练的时候，我们都是使用"><meta name=keywords content="Hugo,theme,even"><meta name=generator content="Hugo 0.121.1 with theme even"><link rel=canonical href=http://zput.github.io/post/_posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link href=/sass/main.min.e291ec317bf4a8f48812dc8bcd749ad9270976917bf2027d01a4415caecb80a5.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="特征降维"><meta property="og:description" content="降维是指在某些限定条件下，降低随机变量（特征）个数，得到一组“不相关”主变量的过程。 降低随机变量的个数 正是因为在进行训练的时候，我们都是使用"><meta property="og:type" content="article"><meta property="og:url" content="http://zput.github.io/post/_posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4/"><meta property="article:section" content="post"><meta property="article:published_time" content="2023-11-02T19:00:00+00:00"><meta property="article:modified_time" content="2023-11-02T19:00:00+00:00"><meta itemprop=name content="特征降维"><meta itemprop=description content="降维是指在某些限定条件下，降低随机变量（特征）个数，得到一组“不相关”主变量的过程。 降低随机变量的个数 正是因为在进行训练的时候，我们都是使用"><meta itemprop=datePublished content="2023-11-02T19:00:00+00:00"><meta itemprop=dateModified content="2023-11-02T19:00:00+00:00"><meta itemprop=wordCount content="3088"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="特征降维"><meta name=twitter:description content="降维是指在某些限定条件下，降低随机变量（特征）个数，得到一组“不相关”主变量的过程。 降低随机变量的个数 正是因为在进行训练的时候，我们都是使用"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>Zput</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/go-goroutine><li class=mobile-menu-item>go源码专栏</li></a><a href=/post/mrmk/><li class=mobile-menu-item>中间件专栏</li></a><a href=/post/cloud_native/><li class=mobile-menu-item>云原生专栏</li></a><a href=/post/postgraduate/><li class=mobile-menu-item>考研专栏</li></a><a href=/post/><li class=mobile-menu-item>时间线</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>Zput</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/go-goroutine>go源码专栏</a></li><li class=menu-item><a class=menu-item-link href=/post/mrmk/>中间件专栏</a></li><li class=menu-item><a class=menu-item-link href=/post/cloud_native/>云原生专栏</a></li><li class=menu-item><a class=menu-item-link href=/post/postgraduate/>考研专栏</a></li><li class=menu-item><a class=menu-item-link href=/post/>时间线</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>特征降维</h1><div class=post-meta><span class=post-time>2023-11-02</span><div class=post-category><a href=/categories/applicationandsoftware/>applicationAndSoftware</a></div></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class="post-toc-content always-active"><nav id=TableOfContents><ul><li><ul><li><a href=#特征选择>特征选择</a><ul><li><a href=#方法>方法</a><ul><li><a href=#过滤式>过滤式</a></li><li><a href=#方差选择法>方差选择法</a></li><li><a href=#pearson相关系数>Pearson相关系数</a></li></ul></li></ul></li><li><a href=#主成分分析>主成分分析</a></li><li><a href=#附录>附录</a><ul><li><a href=#标准差和方差>标准差和方差</a></li><li><a href=#关于0-1分布的方差推导>关于0-1分布的方差推导</a></li></ul></li><li><a href=#参考>参考</a></li></ul></li></ul></nav></div></div><div class=article-container><div class=post-content><p>降维是指在某些限定条件下，降低随机变量（特征）个数，得到一组“不相关”主变量的过程。</p><p>降低<strong>随机变量</strong>的个数</p><blockquote><p>正是因为在进行训练的时候，我们都是使用特征进行学习。如果特征本身存在问题或者特征之间相关性较强，对于算法学习预测会影响较大。</p></blockquote><h2 id=特征选择>特征选择</h2><p>1定义
数据中包含冗余或相关变量（或称特征、属性、指标等），旨在从原有特征中找出主要特征。</p><p><img src=https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/20231118085537.png alt=20231118085537></p><h3 id=方法>方法</h3><ul><li>Filter(过滤法)：按照<strong>发散性</strong>或<strong>相关性</strong>对各个特征进行评分，设定阈值或者待选择特征的个数进行筛选<ul><li><a href=https://www.shuxuele.com/data/standard-deviation.html>方差</a>选择法：低方差特征过滤</li><li>相关系数：特征与特征之间的相关性</li></ul></li><li>Wrapper(包装法)：根据目标函数（往往是预测效果评分），每次选择若干特征，或者排除若干特征</li><li>Embedded(嵌入法)：先使用某些机器学习的模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征（类似于Filter，只不过系数是通过训练得来的）<ul><li>决策树：信息熵、信息增益</li><li>正则化：L1、L2</li><li>深度学习：卷积等</li></ul></li></ul><h4 id=过滤式>过滤式</h4><p>基本想法是：分别对每个特征 x_i ，计算 x_i 相对于类别标签 y 的信息量 S(i) ，得到 n 个结果。然后将 n 个 S(i) 按照从大到小排序，输出前 k 个特征。显然，这样复杂度大大降低。那么关键的问题就是使用什么样的方法来度量 S(i) ，我们的目标是选取与 y 关联最密切的一些 特征x_i 。</p><p>Pearson相关系数
卡方验证
互信息和最大信息系数
距离相关系数
方差选择法</p><h4 id=方差选择法>方差选择法</h4><p>这种方法先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。</p><p>例如，假设我们有一个具有布尔特征的数据集，并且我们要删除超过80％的样本中的一个或零（开或关）的所有特征。布尔特征是伯努利随机变量，这些变量的方差由下式给出:</p><p>VarianceThreshold是特征选择的简单基线方法。它删除方差不符合某个阈值的所有特征。默认情况下，它会删除所有零差异特征，即所有样本中具有相同值的特征。代码如下：</p><ul><li>class原型: sklearn.feature_selection.VarianceThreshold(threshold=0.0)</li><li>参数: threshold: 指定方差的阙值，特征的方差低于此阙值将被剔除</li><li>成员属性：variances_:一个数组，分别为各特征的方差</li><li>成员方法：<ul><li>fit(X,[,y]):从样本数据中学习方差</li><li>transform(X): 执行特征选择（删除低于阙值的特征）</li><li>fit_transform(X [,y]) 执行特征选择（删除低于阙值的特征）</li><li>get_support([indices])</li><li>True:返回被选出的特征的下标</li><li>False:返回一个布尔值组成的数组，该数组指示哪些特征被选中</li></ul></li></ul><p>例子： 假设我们有一个bool型的数据集，并且我们想要去掉那些超过80%的样本都取值为0（或者为1）的所有特征。bool型特征是伯努利随机变量，方差为 $\operatorname{Var}(X)=p(1-p)$</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.feature_selection</span> <span class=kn>import</span> <span class=n>VarianceThreshold</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=p>[[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]]</span>
</span></span><span class=line><span class=cl><span class=c1># 方差选择法，返回值为特征选择后的数据</span>
</span></span><span class=line><span class=cl><span class=c1># 参数threshold为方差的阈值</span>
</span></span><span class=line><span class=cl><span class=n>sel</span> <span class=o>=</span> <span class=n>VarianceThreshold</span><span class=p>(</span><span class=n>threshold</span><span class=o>=</span><span class=p>(</span><span class=mf>.8</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=mf>.8</span><span class=p>)))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>sel</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>输出结果</span><span class=err>：</span>
</span></span><span class=line><span class=cl><span class=n>array</span><span class=p>([[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=err>​</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=err>​</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=err>​</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=err>​</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=err>​</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]])</span> 
</span></span><span class=line><span class=cl><span class=c1># 如预期的那样，VarianceThreshold已经删除了第一列，其具有 p=5/6&gt;0.8 包含零的概率。</span>
</span></span></code></pre></td></tr></table></div></div><p>方差选择的逻辑并不是很合理，这个是基于各特征分布较为接近的时候，才能以方差的逻辑来衡量信息量。但是如果是<strong>离散</strong>的或是<strong>仅集中</strong>在几个数值上，如果分布过于集中，其信息量则较小。而对于连续变量，由于阈值可以连续变化，所以信息量不随方差而变。 实际使用时，可以结合cross-validate进行检验</p><h4 id=pearson相关系数>Pearson相关系数</h4><p>皮尔森相关系数是一种最简单的，能帮助理解特征和响应变量之间关系的方法，该方法衡量的是变量之间的线性相关性，结果的取值区间为[-1，1]，-1表示完全的负相关，+1表示完全的正相关，0表示没有线性相关。</p><ul><li>相关系数的值介于-1与+1之间，即-1≤r≤+1。其性质如下:<ul><li>当>0时，表示两变量正槽关，r&lt;0时，两变量为负相关。</li><li>当|r|=1时，表示两变量为完全相关，当r=0时，表示两变量间无相关关系。</li><li>当0&lt;|r|&lt;1时，表示两变量存在一定程度的相关。且越接近1，两变量间线性关系越密切；越接近于0，表示两变量的线性相关越弱。</li><li>一般可按三级划分：|r|&lt;0.4为低度相关；0.4≤|r|&lt;0.7为显著性相关；0.7≤|r|&lt;1为高度线性相关。</li></ul></li></ul><p>Pearson Correlation速度快、易于计算，经常在拿到数据(经过清洗和特征提取之后的)之后第一时间就执行。Scipy的 pearsonr 方法能够同时计算 相关系数 和p-value.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>scipy.stats</span> <span class=kn>import</span> <span class=n>pearsonr</span>
</span></span><span class=line><span class=cl><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>size</span> <span class=o>=</span> <span class=mi>300</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>size</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span><span class=o>=</span><span class=n>x</span><span class=o>+</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>size</span><span class=p>)</span> <span class=c1>#模拟y</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 输入：特征矩阵和目标向量</span>
</span></span><span class=line><span class=cl><span class=c1># 输出： 相关系数值和p值</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Pearson相关系数:&#34;</span><span class=p>,</span> <span class=n>pearsonr</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><p>Pearson相关系数的一个明显缺陷是，作为特征排序机制，他只对线性关系敏感。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近0。例如：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=n>x</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>,</span><span class=mi>100000</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span> <span class=n>pearsonr</span><span class=p>(</span><span class=n>x</span><span class=p>,</span><span class=n>x</span><span class=o>**</span><span class=mi>2</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=c1># 结果: -0.00230804707612</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=主成分分析>主成分分析</h2><p>主成分分析（Principal Component Analysis，PCA）是一种常用的数据降维技术。其目的是从高维度的数据中提取主要特征，将数据的维度降低到较低的维度，同时尽可能保留大部分原始数据的信息。</p><ul><li>定义：高维数据转化为低维数据的过程，在此过程中可能会舍弃原有数据、创造新的变量</li><li>作用：是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。</li><li>应用：回归分析或者聚类分析当中</li></ul><p>具体来说，主成分分析的过程如下：</p><ol><li>对数据进行标准化处理，使得每个变量的均值为0，方差为1。</li><li>计算数据的协方差矩阵。</li><li>对协方差矩阵进行特征值分解，得到特征值和特征向量。</li><li>将特征值按大小排序，选取最大的k个特征值对应的特征向量作为主成分。</li><li>将原始数据映射到选取的主成分上，得到降维后的数据。</li></ol><p>主成分分析的重点在于计算协方差矩阵和特征值分解。协方差矩阵描述了变量间的线性关系，特征值和特征向量是协方差矩阵的数学特性。在数据降维时，我们通过选取最大的k个特征值对应的特征向量作为主成分，保留了数据中的主要变化方向和信息。由于特征值表示了这些主成分解释变量总方差的比例，因此可以根据特征值的大小选择适当的主成分数量，实现数据的降维。</p><p><img src=https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/xx.jpg alt=xx></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.decomposition</span> <span class=kn>import</span> <span class=n>PCA</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>pca_demo</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    PCA降维
</span></span></span><span class=line><span class=cl><span class=s2>    :return:
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>data</span> <span class=o>=</span> <span class=p>[[</span><span class=mi>2</span><span class=p>,</span><span class=mi>8</span><span class=p>,</span><span class=mi>4</span><span class=p>,</span><span class=mi>5</span><span class=p>],</span> <span class=p>[</span><span class=mi>6</span><span class=p>,</span><span class=mi>3</span><span class=p>,</span><span class=mi>0</span><span class=p>,</span><span class=mi>8</span><span class=p>],</span> <span class=p>[</span><span class=mi>5</span><span class=p>,</span><span class=mi>4</span><span class=p>,</span><span class=mi>9</span><span class=p>,</span><span class=mi>1</span><span class=p>]]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 1、实例化一个转换器类</span>
</span></span><span class=line><span class=cl>    <span class=n>transfer</span> <span class=o>=</span> <span class=n>PCA</span><span class=p>(</span><span class=n>n_components</span><span class=o>=</span><span class=mf>0.95</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 2、调用fit_transform</span>
</span></span><span class=line><span class=cl>    <span class=n>data_new</span> <span class=o>=</span> <span class=n>transfer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;data_new:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>data_new</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>pca_demo</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=附录>附录</h2><h3 id=标准差和方差>标准差和方差</h3><p>差：的意思是离正常有多远。</p><h3 id=关于0-1分布的方差推导>关于0-1分布的方差推导</h3><p>0-1分布是一种特殊的离散概率分布，只有两个可能取值：0 和 1。它也被称为伯努利分布或两点分布。</p><p>对于一个随机变量 $X$，若 $X$ 服从 0-1 分布，则其方差 $\operatorname{Var}(X)$ 的计算如下：</p><p>首先，0-1分布的期望值为 $E(X)=p$，其中 $p$ 是 $X=1$ 的概率，$1-p$ 是 $X=0$ 的概率。</p><p>然后，根据方差的定义：</p><p>$$
\begin{aligned}
\operatorname{Var}(X)&=E[(X-E(X))^2]\
&=E[(X-p)^2]
\end{aligned}
$$</p><p>因为 $X$ 只能取 0 或 1，所以可以将上式展开，得到：</p><p>$$
\begin{aligned}
\operatorname{Var}(X)&=E[(X-p)^2]\
&=E[X^2-2Xp+p^2]\
&=E(X^2)-2pE(X)+p^2\
&=E(X^2)-2p^2+p^2\
&=E(X^2)-p^2
\end{aligned}
$$</p><p>最后我们来计算 $E(X^2)$，根据定义：</p><p>$$ E(X^2)=\sum_{i=0}^{1} i^2 P(X=i) $$</p><p>因为 $X$ 只能取 0 或 1，所以：</p><p>$$
E(X^2)=0^2P(X=0)+1^2P(X=1)=0\times(1-p)+1\times p=p
$$</p><p>带入前面的式子可得：</p><p>$$
\operatorname{Var}(X)=E(X^2)-p^2=p-p^2=p(1-p)
$$</p><p>因此，当 $X$ 服从 0-1 分布时，$\operatorname{Var}(X)=p(1-p)$。</p><h2 id=参考>参考</h2><ul><li><a href=https://zhuanlan.zhihu.com/p/74198735>https://zhuanlan.zhihu.com/p/74198735</a></li></ul><hr><p>(x mean)/std
fit_transform()
fit() 计算每一列的平均值、标准差
transform() 带入(X-mean)/std进行最终的转换</p><p><img src=https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/20231104181629.png alt=20231104181629></p><p>3.1 sklearn转换器和估计器
转换器
估计器(estimator)
3.1.1 转换器 - 特征工程的父类
1 实例化 (实例化的是一个转换器类(Transformer))
2 调用fit_transform(对于文档建立分类词频矩阵，不能同时调用)
标准化：
(x - mean) / std
fit_transform()
fit() 计算 每一列的平均值、标准差
transform() (x - mean) / std进行最终的转换
3.1.2 估计器(sklearn机器学习算法的实现)
估计器(estimator)
1 实例化一个estimator
2 estimator.fit(x_train, y_train) 计算
—— 调用完毕，模型生成
3 模型评估：
1）直接比对真实值和预测值
y_predict = estimator.predict(x_test)
y_test == y_predict
2）计算准确率
accuracy = estimator.score(x_test, y_test)</p></div></div><div class=post-copyright><p class=copyright-item><span class=item-title>文章作者</span>
<span class=item-content>zput</span></p><p class=copyright-item><span class=item-title>上次更新</span>
<span class=item-content>2023-11-02</span></p></div><footer class=post-footer><nav class=post-nav><a class=prev href=/post/_posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/><i class="iconfont icon-left"></i>
<span class="prev-text nav-default">朴素贝叶斯算法</span>
<span class="prev-text nav-mobile">上一篇</span>
</a><a class=next href=/post/_posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/tfidf/><span class="next-text nav-default">特征提取之字典提取与文本提取</span>
<span class="next-text nav-mobile">下一篇</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div><script src=https://giscus.app/client.js data-repo=zput/utterances-comments data-repo-id=R_kgDOHhATGQ data-category=Announcements data-category-id=DIC_kwDOHhATGc4CPxca data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=1 data-input-position=top data-theme=light data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:wkzxc@sina.com class="iconfont icon-email" title=email></a><a href=https://github.com/zput class="iconfont icon-github" title=github></a><a href=http://zput.github.io/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=copyright-year>&copy;
2017 -
2023<span class=heart><i class="iconfont icon-heart"></i></span><span>zput</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script><script type=text/javascript src=/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js></script><script type=text/javascript>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script></body></html>