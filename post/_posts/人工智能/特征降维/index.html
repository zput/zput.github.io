<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>特征降维 - Zput's blog</title>
<meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=Cache-Control content="no-transform"><meta http-equiv=Cache-Control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="zput"><meta name=description content="降维是指在某些限定条件下，降低随机变量（特征）个数，得到一组“不相关”主变量的过程。 降低随机变量的个数 正是因为在进行训练的时候，我们都是使用"><meta name=keywords content="Hugo,theme,even"><meta name=generator content="Hugo 0.120.4 with theme even"><link rel=canonical href=http://zput.github.io/post/_posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link href=/sass/main.min.e291ec317bf4a8f48812dc8bcd749ad9270976917bf2027d01a4415caecb80a5.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="特征降维"><meta property="og:description" content="降维是指在某些限定条件下，降低随机变量（特征）个数，得到一组“不相关”主变量的过程。 降低随机变量的个数 正是因为在进行训练的时候，我们都是使用"><meta property="og:type" content="article"><meta property="og:url" content="http://zput.github.io/post/_posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4/"><meta property="article:section" content="post"><meta property="article:published_time" content="2023-11-02T19:00:00+00:00"><meta property="article:modified_time" content="2023-11-02T19:00:00+00:00"><meta itemprop=name content="特征降维"><meta itemprop=description content="降维是指在某些限定条件下，降低随机变量（特征）个数，得到一组“不相关”主变量的过程。 降低随机变量的个数 正是因为在进行训练的时候，我们都是使用"><meta itemprop=datePublished content="2023-11-02T19:00:00+00:00"><meta itemprop=dateModified content="2023-11-02T19:00:00+00:00"><meta itemprop=wordCount content="1256"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="特征降维"><meta name=twitter:description content="降维是指在某些限定条件下，降低随机变量（特征）个数，得到一组“不相关”主变量的过程。 降低随机变量的个数 正是因为在进行训练的时候，我们都是使用"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>Zput</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/go-goroutine><li class=mobile-menu-item>go源码专栏</li></a><a href=/post/mrmk/><li class=mobile-menu-item>中间件专栏</li></a><a href=/post/cloud_native/><li class=mobile-menu-item>云原生专栏</li></a><a href=/post/postgraduate/><li class=mobile-menu-item>考研专栏</li></a><a href=/post/><li class=mobile-menu-item>时间线</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>Zput</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/go-goroutine>go源码专栏</a></li><li class=menu-item><a class=menu-item-link href=/post/mrmk/>中间件专栏</a></li><li class=menu-item><a class=menu-item-link href=/post/cloud_native/>云原生专栏</a></li><li class=menu-item><a class=menu-item-link href=/post/postgraduate/>考研专栏</a></li><li class=menu-item><a class=menu-item-link href=/post/>时间线</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>特征降维</h1><div class=post-meta><span class=post-time>2023-11-02</span><div class=post-category><a href=/categories/applicationandsoftware/>applicationAndSoftware</a></div></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class="post-toc-content always-active"><nav id=TableOfContents><ul><li><ul><li><a href=#特征选择>特征选择</a><ul><li><a href=#方法>方法</a></li><li><a href=#方差选择法>方差选择法</a></li></ul></li><li><a href=#主成分分析>主成分分析</a></li><li><a href=#附录>附录</a></li></ul></li></ul></nav></div></div><div class=article-container><div class=post-content><p>降维是指在某些限定条件下，降低随机变量（特征）个数，得到一组“不相关”主变量的过程。</p><p>降低<strong>随机变量</strong>的个数</p><blockquote><p>正是因为在进行训练的时候，我们都是使用特征进行学习。如果特征本身存在问题或者特征之间相关性较强，对于算法学习预测会影响较大。</p></blockquote><h2 id=特征选择>特征选择</h2><p>1定义
数据中包含冗余或相关变量（或称特征、属性、指标等），旨在从原有特征中找出主要特征。</p><p><img src=https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/20231118085537.png alt=20231118085537></p><h3 id=方法>方法</h3><ul><li>Fite(过滤式)：主要探究特征本身特点、特征与特征和目标值之间关联<ul><li><a href=https://www.shuxuele.com/data/standard-deviation.html>方差</a>选择法：低方差特征过滤</li><li>相关系数：特征与特征之间的相关性</li></ul></li><li>Embedded(嵌入式)：算法自动选择特征（特征与目标值之间的关联）<ul><li>决策树：信息熵、信息增益</li><li>正则化：L1、L2</li><li>深度学习：卷积等</li></ul></li></ul><p>过滤特征选择法还有一种方法不需要度量特征 x_i 和类别标签 y 的信息量。</p><h3 id=方差选择法>方差选择法</h3><p>这种方法先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。</p><p>例如，假设我们有一个具有布尔特征的数据集，并且我们要删除超过80％的样本中的一个或零（开或关）的所有特征。布尔特征是伯努利随机变量，这些变量的方差由下式给出:</p><p>VarianceThreshold是特征选择的简单基线方法。它删除方差不符合某个阈值的所有特征。默认情况下，它会删除所有零差异特征，即所有样本中具有相同值的特征。代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>- sklearn.feature_selection.VarianceThreshold(threshold 0.0)
</span></span><span class=line><span class=cl>  - 删除所有低方差特征
</span></span><span class=line><span class=cl>- Variance.fit_transform(X)
</span></span><span class=line><span class=cl>  - X：代表numpy array格式的数据[n_samples,n_features]。
</span></span><span class=line><span class=cl>  - 返回值：训练集差异低于threshold的特征将被删除。默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征。
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.feature_selection</span> <span class=kn>import</span> <span class=n>VarianceThreshold</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=p>[[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]]</span>
</span></span><span class=line><span class=cl><span class=c1># 方差选择法，返回值为特征选择后的数据</span>
</span></span><span class=line><span class=cl><span class=c1># 参数threshold为方差的阈值</span>
</span></span><span class=line><span class=cl><span class=n>sel</span> <span class=o>=</span> <span class=n>VarianceThreshold</span><span class=p>(</span><span class=n>threshold</span><span class=o>=</span><span class=p>(</span><span class=mf>.8</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=mf>.8</span><span class=p>)))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>sel</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>输出结果</span><span class=err>：</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>array</span><span class=p>([[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=err>​</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=err>​</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=err>​</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=err>​</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=err>​</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]])</span> <span class=n>如预期的那样</span><span class=err>，</span><span class=n>VarianceThreshold已经删除了第一列</span><span class=err>，</span><span class=n>其具有</span> <span class=n>p</span><span class=o>=</span><span class=mi>5</span><span class=o>/</span><span class=mi>6</span><span class=o>&gt;</span><span class=mf>0.8</span> <span class=n>包含零的概率</span><span class=err>。</span>
</span></span></code></pre></td></tr></table></div></div><p>方差选择的逻辑并不是很合理，这个是基于各特征分布较为接近的时候，才能以方差的逻辑来衡量信息量。但是如果是离散的或是仅集中在几个数值上，如果分布过于集中，其信息量则较小。而对于连续变量，由于阈值可以连续变化，所以信息量不随方差而变。 实际使用时，可以结合cross-validate进行检验</p><h2 id=主成分分析>主成分分析</h2><h2 id=附录>附录</h2><p>标准差和方差: 差的意思是离正常有多远</p><hr><p>(x mean)/std
fit_transform()
fit() 计算每一列的平均值、标准差
transform() 带入(X-mean)/std进行最终的转换</p><p><img src=https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/20231104181629.png alt=20231104181629></p><p>3.1 sklearn转换器和估计器
转换器
估计器(estimator)
3.1.1 转换器 - 特征工程的父类
1 实例化 (实例化的是一个转换器类(Transformer))
2 调用fit_transform(对于文档建立分类词频矩阵，不能同时调用)
标准化：
(x - mean) / std
fit_transform()
fit() 计算 每一列的平均值、标准差
transform() (x - mean) / std进行最终的转换
3.1.2 估计器(sklearn机器学习算法的实现)
估计器(estimator)
1 实例化一个estimator
2 estimator.fit(x_train, y_train) 计算
—— 调用完毕，模型生成
3 模型评估：
1）直接比对真实值和预测值
y_predict = estimator.predict(x_test)
y_test == y_predict
2）计算准确率
accuracy = estimator.score(x_test, y_test)</p></div></div><div class=post-copyright><p class=copyright-item><span class=item-title>文章作者</span>
<span class=item-content>zput</span></p><p class=copyright-item><span class=item-title>上次更新</span>
<span class=item-content>2023-11-02</span></p></div><footer class=post-footer><nav class=post-nav><a class=next href=/post/_posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/tfidf/><span class="next-text nav-default">特征提取之字典提取与文本提取</span>
<span class="next-text nav-mobile">下一篇</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div><script src=https://giscus.app/client.js data-repo=zput/utterances-comments data-repo-id=R_kgDOHhATGQ data-category=Announcements data-category-id=DIC_kwDOHhATGc4CPxca data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=1 data-input-position=top data-theme=light data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:wkzxc@sina.com class="iconfont icon-email" title=email></a><a href=https://github.com/zput class="iconfont icon-github" title=github></a><a href=http://zput.github.io/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=copyright-year>&copy;
2017 -
2023<span class=heart><i class="iconfont icon-heart"></i></span><span>zput</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script><script type=text/javascript src=/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js></script><script type=text/javascript>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script></body></html>