<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>特征提取之字典提取与文本提取 - Zput's blog</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="zput"><meta name=description content="字典特征提取 将类别中的特征进行one-hot编码处理。 key value 类别 one-hot编码 独热编码(one-hot) one-hot编码用于将离散的分类"><meta name=keywords content="Hugo,theme,even"><meta name=generator content="Hugo 0.114.0 with theme even"><link rel=canonical href=http://zput.github.io/post/_posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/tfidf/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link href=/sass/main.min.e291ec317bf4a8f48812dc8bcd749ad9270976917bf2027d01a4415caecb80a5.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="特征提取之字典提取与文本提取"><meta property="og:description" content="字典特征提取 将类别中的特征进行one-hot编码处理。 key value 类别 one-hot编码 独热编码(one-hot) one-hot编码用于将离散的分类"><meta property="og:type" content="article"><meta property="og:url" content="http://zput.github.io/post/_posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/tfidf/"><meta property="article:section" content="post"><meta property="article:published_time" content="2023-11-02T19:00:00+00:00"><meta property="article:modified_time" content="2023-11-02T19:00:00+00:00"><meta itemprop=name content="特征提取之字典提取与文本提取"><meta itemprop=description content="字典特征提取 将类别中的特征进行one-hot编码处理。 key value 类别 one-hot编码 独热编码(one-hot) one-hot编码用于将离散的分类"><meta itemprop=datePublished content="2023-11-02T19:00:00+00:00"><meta itemprop=dateModified content="2023-11-02T19:00:00+00:00"><meta itemprop=wordCount content="2273"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="特征提取之字典提取与文本提取"><meta name=twitter:description content="字典特征提取 将类别中的特征进行one-hot编码处理。 key value 类别 one-hot编码 独热编码(one-hot) one-hot编码用于将离散的分类"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>Zput</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/go-goroutine><li class=mobile-menu-item>go源码专栏</li></a><a href=/post/mrmk/><li class=mobile-menu-item>中间件专栏</li></a><a href=/post/cloud_native/><li class=mobile-menu-item>云原生专栏</li></a><a href=/post/postgraduate/><li class=mobile-menu-item>考研专栏</li></a><a href=/post/><li class=mobile-menu-item>时间线</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>Zput</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/go-goroutine>go源码专栏</a></li><li class=menu-item><a class=menu-item-link href=/post/mrmk/>中间件专栏</a></li><li class=menu-item><a class=menu-item-link href=/post/cloud_native/>云原生专栏</a></li><li class=menu-item><a class=menu-item-link href=/post/postgraduate/>考研专栏</a></li><li class=menu-item><a class=menu-item-link href=/post/>时间线</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>特征提取之字典提取与文本提取</h1><div class=post-meta><span class=post-time>2023-11-02</span><div class=post-category><a href=/categories/applicationandsoftware/>applicationAndSoftware</a></div></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class="post-toc-content always-active"><nav id=TableOfContents><ul><li><ul><li><a href=#字典特征提取>字典特征提取</a><ul><li><a href=#独热编码one-hot>独热编码(one-hot)</a><ul><li><a href=#离散的分类>离散的分类</a></li><li><a href=#二进制向量>二进制向量</a></li><li><a href=#举例>举例</a></li></ul></li><li><a href=#示例>示例</a></li></ul></li><li><a href=#文本特征提取>文本特征提取</a><ul><li><a href=#词频统计countvectorizer>词频统计(CountVectorizer)</a></li><li><a href=#词频-逆文件频率tfidfvectorvizer>词频-逆文件频率(TfidfVectorvizer)</a><ul><li><a href=#词频term-frequency>词频(Term Frequency)</a></li><li><a href=#逆文件频率inverse-document-frequency>逆文件频率(Inverse Document Frequency)</a></li><li><a href=#总结>总结</a></li></ul></li></ul></li><li><a href=#参考>参考</a></li></ul></li></ul></nav></div></div><div class=article-container><div class=post-content><h2 id=字典特征提取>字典特征提取</h2><p>将类别中的特征进行one-hot编码处理。</p><table><thead><tr><th>key</th><th>value</th></tr></thead><tbody><tr><td>类别</td><td>one-hot编码</td></tr></tbody></table><h3 id=独热编码one-hot>独热编码(one-hot)</h3><ul><li>one-hot编码用于将离散的分类标签转换为二进制向量，这里的关键词有两个:<ul><li>第一个是离散的分类</li><li>另一个是二进制向量</li></ul></li></ul><h4 id=离散的分类>离散的分类</h4><p>Mnist手写数字识别中，分类是0-9这十个数字。虽然这10个数字是有大小区分的，但对于分类任务来说，数字0和数字1并不存在大小关系。它们仅仅是一种类别，只不过手写数字识别任务中，类别刚好是数字。这就是离散分类。</p><h4 id=二进制向量>二进制向量</h4><p>向量我们都能理解，[1, 2, 3, 4] 这是一个一维数组，也可以称之为一维向量。那么二进制向量，就是里面的数字都是二进制的，像是[0, 1, 0, 0]。</p><p>假设需要对黑人、黄种人、白人这三个类别进行分类。最容易想到的，便是以0代表黑人，以1代表黄种人，以2代表白人这种简单粗暴的方式。但这样不行。分类标签一个重要的作用，就是要计算预测标签与真实标签之间的相似性，从而计算loss值。loss值越小，说明预测标签与真实标签之间越接近。</p><p>相似性其实就是两个标签之间的距离。如果按照0代表黑人，以1代表黄种人，以2代表白人这种表示方法，那么黑人和黄种人之间距离差为1， 黄种人和黑人之间距离为1， 而黑人和白人之间距离为2。
这样在计算损失的时候是完全不能接受的：互相独立的标签之间，出现了不对等的情况。</p><p>因此，需要有一种表示方法，将互相独立的标签表示为互相独立的数字，并且数字之间的距离也相等。</p><p>这就是one-hot，也叫独热编码。
它就用二进制向量来表征这种离散的分类标签。</p><h4 id=举例>举例</h4><table><thead><tr><th>类别</th><th>one-hot编码</th><th></th><th></th></tr></thead><tbody><tr><td>黑人</td><td>1</td><td>0</td><td>0</td></tr><tr><td>黄种人</td><td>0</td><td>1</td><td>0</td></tr><tr><td>白人</td><td>0</td><td>0</td><td>1</td></tr></tbody></table><p>这样得到的编码都是独立的。比如黑人标签，得到一个编码[1, 0, 0]。</p><p>在三维坐标系下，[1, 0, 0]、[0, 1, 0]和[0, 0, 1]这三个向量是互相垂直的，也就是互相正交独立。</p><p>他们之间距离差相等，这就解决了上面说的独立的标签之间，表示方法不对等的情况。</p><h3 id=示例>示例</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>from sklearn.feature_extraction import DictVectorizer
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>def dict_demo():
</span></span><span class=line><span class=cl>    &#34;&#34;&#34;
</span></span><span class=line><span class=cl>    字典特征抽取
</span></span><span class=line><span class=cl>    :return:
</span></span><span class=line><span class=cl>    &#34;&#34;&#34;
</span></span><span class=line><span class=cl>    data = [{&#39;city&#39;: &#39;北京&#39;,&#39;temperature&#39;:100}, {&#39;city&#39;: &#39;上海&#39;,&#39;temperature&#39;:60}, {&#39;city&#39;: &#39;深圳&#39;,&#39;temperature&#39;:30}]
</span></span><span class=line><span class=cl>    # 1、实例化一个转换器类
</span></span><span class=line><span class=cl>    transfer = DictVectorizer(sparse=True)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    # 2、调用fit_transform()
</span></span><span class=line><span class=cl>    data_new = transfer.fit_transform(data)
</span></span><span class=line><span class=cl>    print(&#34;data_new:\n&#34;, data_new.toarray(), type(data_new))
</span></span><span class=line><span class=cl>    print(&#34;特征名字：\n&#34;, transfer.get_feature_names_out())
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    return None
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>if __name__ == &#34;__main__&#34;:
</span></span><span class=line><span class=cl>    # 字典特征抽取
</span></span><span class=line><span class=cl>    dict_demo()
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>data_new:
</span></span><span class=line><span class=cl> <span class=o>[[</span>  0.   1.   0. 100.<span class=o>]</span>
</span></span><span class=line><span class=cl> <span class=o>[</span>  1.   0.   0.  60.<span class=o>]</span>
</span></span><span class=line><span class=cl> <span class=o>[</span>  0.   0.   1.  30.<span class=o>]]</span> &lt;class <span class=s1>&#39;scipy.sparse._csr.csr_matrix&#39;</span>&gt;
</span></span><span class=line><span class=cl>特征名字：
</span></span><span class=line><span class=cl> <span class=o>[</span><span class=s1>&#39;city=上海&#39;</span> <span class=s1>&#39;city=北京&#39;</span> <span class=s1>&#39;city=深圳&#39;</span> <span class=s1>&#39;temperature&#39;</span><span class=o>]</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=文本特征提取>文本特征提取</h2><p>从文本中提取出计算机能识别的数字特征</p><h3 id=词频统计countvectorizer>词频统计(CountVectorizer)</h3><p>统计每个特征值出现的次数(每个词都是一个特征值)</p><p>通常使用CountVectorizer类会将文本中的词语转换为词频矩阵。 例如矩阵中包含一个元素a[i][j]，它表示j词在i类文本下的词频。它通过fit_transform函数计算各个词语出现的次数，通过get_feature_names()可获取词袋中所有文本的关键字，通过toarray()可看到词频矩阵的结果。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.feature_extraction.text</span> <span class=kn>import</span> <span class=n>CountVectorizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>count_demo</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    文本特征抽取：CountVecotrizer
</span></span></span><span class=line><span class=cl><span class=s2>    :return:
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>data</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;life is short,i like like python&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;life is too long,i dislike python&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;talk is cheap, show me my code&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=c1># 1、实例化一个转换器类</span>
</span></span><span class=line><span class=cl>    <span class=n>transfer</span> <span class=o>=</span> <span class=n>CountVectorizer</span><span class=p>(</span><span class=n>stop_words</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;is&#34;</span><span class=p>,</span> <span class=s2>&#34;too&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 2、调用fit_transform</span>
</span></span><span class=line><span class=cl>    <span class=n>data_new</span> <span class=o>=</span> <span class=n>transfer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;特征矩阵：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>data_new</span><span class=o>.</span><span class=n>toarray</span><span class=p>())</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;特征名字：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>transfer</span><span class=o>.</span><span class=n>get_feature_names_out</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># 文本特征抽取：CountVecotrizer</span>
</span></span><span class=line><span class=cl>    <span class=n>count_demo</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>特征矩阵:
</span></span><span class=line><span class=cl> <span class=o>[[</span><span class=m>0</span> <span class=m>0</span> <span class=m>0</span> <span class=m>1</span> <span class=m>2</span> <span class=m>0</span> <span class=m>0</span> <span class=m>0</span> <span class=m>1</span> <span class=m>1</span> <span class=m>0</span> 0<span class=o>]</span>
</span></span><span class=line><span class=cl> <span class=o>[</span><span class=m>0</span> <span class=m>0</span> <span class=m>1</span> <span class=m>1</span> <span class=m>0</span> <span class=m>1</span> <span class=m>0</span> <span class=m>0</span> <span class=m>1</span> <span class=m>0</span> <span class=m>0</span> 0<span class=o>]</span>
</span></span><span class=line><span class=cl> <span class=o>[</span><span class=m>1</span> <span class=m>1</span> <span class=m>0</span> <span class=m>0</span> <span class=m>0</span> <span class=m>0</span> <span class=m>1</span> <span class=m>1</span> <span class=m>0</span> <span class=m>0</span> <span class=m>1</span> 1<span class=o>]]</span>
</span></span><span class=line><span class=cl>特征名字：
</span></span><span class=line><span class=cl> <span class=o>[</span><span class=s1>&#39;cheap&#39;</span> <span class=s1>&#39;code&#39;</span> <span class=s1>&#39;dislike&#39;</span> <span class=s1>&#39;life&#39;</span> <span class=s1>&#39;like&#39;</span> <span class=s1>&#39;long&#39;</span> <span class=s1>&#39;me&#39;</span> <span class=s1>&#39;my&#39;</span> <span class=s1>&#39;python&#39;</span> <span class=s1>&#39;short&#39;</span>
</span></span><span class=line><span class=cl> <span class=s1>&#39;show&#39;</span> <span class=s1>&#39;talk&#39;</span><span class=o>]</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=词频-逆文件频率tfidfvectorvizer>词频-逆文件频率(TfidfVectorvizer)</h3><p>TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文件频率)都带有频率，具体含义是指：一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章。</p><h4 id=词频term-frequency>词频(Term Frequency)</h4><p>TF(Term Frequency, 词频)表示词条在文本中出现的频率。</p><p>$$ TF_{i, j}=\frac{n_{i, j}}{\sum_{k} n_{k, j}}$$</p><p>其中, $n_{i, j}$ 表示词条 $t_{i}$ 在文档 $doc_{j}$ 中出现的次数, $TF_{i, j}$ 就是表示词条 $t_{i}$ 在文档 $doc_{j}$ 中出现的频率。</p><h4 id=逆文件频率inverse-document-frequency>逆文件频率(Inverse Document Frequency)</h4><p>IDF(Inverse Document Frequency, 逆<strong>文件</strong>频率)表示关键词的普遍程度。如果包含此词条的<strong>文档</strong>越少，IDF越大，则说明此词条具有很好的类别区分能力</p><p>$$ IDF_{i}=\log \frac{|D|}{1+\left|j: t_{i} \in doc_{j}\right|} $$</p><ul><li>其中:<ul><li>$|D|$ 表示所有文档的数量。</li><li>$\left|j: t_{i} \in doc_{j}\right|$ 表示包含词条 $t_{i}$ 的文档数量<ul><li>为什么这里要加 1 呢? 主要是防止包含词条 $t_{i}$ 的数量为 0 从而导致运算出错的现象发生。</li></ul></li></ul></li></ul><p>某一特定文件内的高词语频率, 以及该词语在整个文件集合中的低文件频率, 可以产生出高权重的 TF-IDF。因此, TF-IDF倾向于过滤掉常见的词语, 保留重要的词语, 表达为 $TF-IDF=TF\cdot IDF$</p><h4 id=总结>总结</h4><p>TF-IDF：采用一种统计方法，根据字词的在文本中<strong>出现的次数</strong>和在整个语料中包含此字词的文档倒频率来计算一个字词在<strong>整个语料中的重要程度</strong>。它的优点是能过滤掉一些常见的却无关紧要本的词语，同时保留影响整个文本的重要字词。</p><p>TF（Term Frequency）表示某个字词在整篇文章中出现的频率。
IDF（InversDocument Frequency）表示计算倒文本频率。文本频率是指包含某个字词所有文档占在整个语料所有文档之比。倒文档频率又称为逆文档频率，它是文档频率的倒数，主要用于降低所有文档中一些常见却对文档影响不大的词语的作用。</p><h2 id=参考>参考</h2><p><a href=https://zhuanlan.zhihu.com/p/97273457>https://zhuanlan.zhihu.com/p/97273457</a>
<a href="https://www.zhihu.com/tardis/zm/art/268886634?source_id=1005">https://www.zhihu.com/tardis/zm/art/268886634?source_id=1005</a>
<a href=https://zhuanlan.zhihu.com/p/634296763>https://zhuanlan.zhihu.com/p/634296763</a></p></div></div><div class=post-copyright><p class=copyright-item><span class=item-title>文章作者</span>
<span class=item-content>zput</span></p><p class=copyright-item><span class=item-title>上次更新</span>
<span class=item-content>2023-11-02</span></p></div><footer class=post-footer><nav class=post-nav><a class=prev href=/post/_posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4/><i class="iconfont icon-left"></i>
<span class="prev-text nav-default">特征降维</span>
<span class="prev-text nav-mobile">上一篇</span></a>
<a class=next href=/post/_posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/machine_study/><span class="next-text nav-default">特征预处理</span>
<span class="next-text nav-mobile">下一篇</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div><script src=https://giscus.app/client.js data-repo=zput/utterances-comments data-repo-id=R_kgDOHhATGQ data-category=Announcements data-category-id=DIC_kwDOHhATGc4CPxca data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=1 data-input-position=top data-theme=light data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:zput@aliyun.com class="iconfont icon-email" title=email></a>
<a href=https://github.com/zput class="iconfont icon-github" title=github></a>
<a href=http://zput.github.io/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=copyright-year>&copy;
2017 -
2025<span class=heart><i class="iconfont icon-heart"></i></span><span>zput</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js></script>
<script type=text/javascript>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script></body></html>