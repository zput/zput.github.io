<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>线性回归和逻辑回归 - Zput's blog</title>
<meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=Cache-Control content="no-transform"><meta http-equiv=Cache-Control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="zput"><meta name=description content="线性回归 线性回归(Linear regression)是利用回归方程（函数）对一个或多个自变量（特征值）和因变量（目标值）之间关系进行建模的一"><meta name=keywords content="Hugo,theme,even"><meta name=generator content="Hugo 0.120.4 with theme even"><link rel=canonical href=http://zput.github.io/post/_posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link href=/sass/main.min.e291ec317bf4a8f48812dc8bcd749ad9270976917bf2027d01a4415caecb80a5.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="线性回归和逻辑回归"><meta property="og:description" content="线性回归 线性回归(Linear regression)是利用回归方程（函数）对一个或多个自变量（特征值）和因变量（目标值）之间关系进行建模的一"><meta property="og:type" content="article"><meta property="og:url" content="http://zput.github.io/post/_posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"><meta property="article:section" content="post"><meta property="article:published_time" content="2023-11-09T19:00:00+00:00"><meta property="article:modified_time" content="2023-11-09T19:00:00+00:00"><meta itemprop=name content="线性回归和逻辑回归"><meta itemprop=description content="线性回归 线性回归(Linear regression)是利用回归方程（函数）对一个或多个自变量（特征值）和因变量（目标值）之间关系进行建模的一"><meta itemprop=datePublished content="2023-11-09T19:00:00+00:00"><meta itemprop=dateModified content="2023-11-09T19:00:00+00:00"><meta itemprop=wordCount content="5828"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="线性回归和逻辑回归"><meta name=twitter:description content="线性回归 线性回归(Linear regression)是利用回归方程（函数）对一个或多个自变量（特征值）和因变量（目标值）之间关系进行建模的一"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>Zput</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/go-goroutine><li class=mobile-menu-item>go源码专栏</li></a><a href=/post/mrmk/><li class=mobile-menu-item>中间件专栏</li></a><a href=/post/cloud_native/><li class=mobile-menu-item>云原生专栏</li></a><a href=/post/postgraduate/><li class=mobile-menu-item>考研专栏</li></a><a href=/post/><li class=mobile-menu-item>时间线</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>Zput</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/go-goroutine>go源码专栏</a></li><li class=menu-item><a class=menu-item-link href=/post/mrmk/>中间件专栏</a></li><li class=menu-item><a class=menu-item-link href=/post/cloud_native/>云原生专栏</a></li><li class=menu-item><a class=menu-item-link href=/post/postgraduate/>考研专栏</a></li><li class=menu-item><a class=menu-item-link href=/post/>时间线</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>线性回归和逻辑回归</h1><div class=post-meta><span class=post-time>2023-11-09</span><div class=post-category><a href=/categories/applicationandsoftware/>applicationAndSoftware</a></div></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class="post-toc-content always-active"><nav id=TableOfContents><ul><li><ul><li><a href=#线性回归>线性回归</a><ul><li><a href=#损失函数>损失函数</a></li><li><a href=#优化算法>优化算法</a><ul><li><a href=#正规方程>正规方程</a><ul><li><a href=#sklearn实现正规方程>sklearn实现正规方程</a></li></ul></li><li><a href=#梯度下降>梯度下降</a><ul><li><a href=#sklearn实现梯度下降>sklearn实现梯度下降</a></li></ul></li></ul></li><li><a href=#模型评估>模型评估</a></li><li><a href=#举例>举例</a></li></ul></li><li><a href=#欠拟合与过拟合>欠拟合与过拟合</a><ul><li><a href=#正则化类别>正则化类别</a></li></ul></li><li><a href=#线性回归的改进-岭回归>线性回归的改进-岭回归</a></li><li><a href=#逻辑回归>逻辑回归</a><ul><li><a href=#原理>原理</a><ul><li><a href=#输入>输入</a></li><li><a href=#激活函数>激活函数</a></li><li><a href=#损失函数-1>损失函数</a></li><li><a href=#优化损失>优化损失</a></li></ul></li><li><a href=#逻辑回归api>逻辑回归API</a></li></ul></li><li><a href=#模型评估二分类>模型评估(二分类)</a><ul><li><a href=#精确率与召回率>精确率与召回率</a></li><li><a href=#分类评估报告api>分类评估报告API</a></li><li><a href=#如何衡量样本不均衡下的评估>如何衡量样本不均衡下的评估？</a><ul><li><a href=#roc曲线与auc指标>ROC曲线与AUC指标</a></li></ul></li></ul></li></ul></li></ul></nav></div></div><div class=article-container><div class=post-content><h2 id=线性回归>线性回归</h2><p>线性回归(Linear regression)是利用回归方程（函数）对一个或多个自变量（特征值）和因变量（目标值）之间关系进行建模的一种分析方式。
特点：只有一个自变量的情况称为单变量回归，多于一个自变量情况的叫做多元回归</p><p>通用公式:
$$
h(w)=w_{1} x_{1}+w_{2} x_{2}+w_{3} x_{3} \ldots+\mathrm{b}=w^{T} x+b
$$</p><p>其中 w,x可以理解为矩阵:
$$
w=\left(\begin{array}{c}
b \
w_{1} \
w_{2}
\end{array}\right), x=\left(\begin{array}{c}
1 \
x_{1} \
x_{2}
\end{array}\right)
$$</p><p>那么怎么理解呢？我们来看几个例子</p><ul><li>期末成绩：0.7×考试成绩+0.3x平时成绩</li><li>房子价格=0.02×中心区域的距离+0.04×城市一氧化氮浓度+(-0.12×自住房平均房价)+0.254×城镇犯罪率
上面两个例子，我们看到特征值与目标值之间建立了一个关系，这个关系可以理解为线性模型。</li></ul><p>线性回归当中线性模型有两种，一种是线性关系，另一种是非线性关系。在这里我们只能画一个平面更好去理解，所以都用单个特征或两个特征举例子。</p><p align=middle><img src=https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/20231126213321.png width=20% height=20% alt=效果图></p><p align=middle><img src=https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/20231126213349.png width=20% height=20% alt=效果图></p><blockquote><p>注释：单特征与目标值的关系呈直线关系，或者两个特征与目标值呈现平面的关系更高维度的我们不用自己去想，记住这种关系即可</p></blockquote><p align=middle><img src=https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/20231126213436.png width=20% height=20% alt=效果图></p><blockquote><p>注：线性关系一定是线性模型，线性模型不一定是线性关系</p></blockquote><h3 id=损失函数>损失函数</h3><p>假设刚才的房子例子，真实的数据之间存在这样的关系</p><blockquote><p>真实关系：真实房子价格=0.02×中心区域的距离+0.04×城市一氧化氮浓度+（-0.12×自
那么现在呢，我们随意指定一个关系(猜测)
随机指定关系：预测房子价格=0.25×中心区域的距离+0.14×城市一氧化氮浓度+0,42×
请问这样的话，会发生什么？真实结果与我们预测的结果之间是不是存在一定的误差呢？类似这样样子</p></blockquote><p align=middle><img src=https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/20231126213621.png width=20% height=20% alt=效果图></p><p>即：目标就是找到一条直线，使所有点到直线的距离之和最小，即误差最小</p><p>$$
\begin{aligned}
J(\theta) & =\left(h_{w}\left(x_{1}\right)-y_{1}\right)^{2}+\left(h_{w}\left(x_{2}\right)-y_{2}\right)^{2}+\cdots+\left(h_{w}\left(x_{m}\right)-y_{m}\right)^{2} \
& =\sum_{i=1}^{m}\left(h_{w}\left(x_{i}\right)-y_{i}\right)^{2}
\end{aligned}
$$</p><ul><li>$y_i$为第i个训练样本的真实值。</li><li>$h(x_i)$为第i个训练样本特征值组合预测函数,又称最小二乘法。</li></ul><p>如何去减少这个损失，使我们预测的更加准确些？既然存在了这个损失，我们一直说机器学习有自动学习的功能，在线性回归这里更是能够体现。这里可以通过一些优化方法去优化（其实是数学当中的求导功能）回归的总损失</p><h3 id=优化算法>优化算法</h3><p>如何去求模型当中的W,使得损失最小？（目的是找到最小损失对应的W值）
线性回归经常使用的两种优化算法。</p><h4 id=正规方程>正规方程</h4><p>$$
w=\left(X^{T} X\right)^{-1} X^{T} y
$$</p><blockquote><p>理解：X为特征值矩阵，y为目标值矩阵。直接求到最好的结果
缺点：当特征过多过复杂时，求解速度太慢并且得不到结果</p></blockquote><p align=middle><img src=https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/20231126214255.png width=20% height=20% alt=效果图></p><h5 id=sklearn实现正规方程>sklearn实现正规方程</h5><ul><li>sklearn.linear_model.LinearRegression(fit_intercept=True)<ul><li>通过正规方程优化:<ul><li>fit_intercept:是否计算偏置</li><li>LinearRegression.coef_:回归系数</li><li>LinearRegression.intercept:偏置</li></ul></li></ul></li></ul><h4 id=梯度下降>梯度下降</h4><p>$$
\begin{array}{l}
w_{1}:=w_{1}-\alpha \frac{\partial \operatorname{cost}\left(w_{0}+w_{1} x_{1}\right)}{\partial w_{1}} \
w_{0}:=w_{0}-\alpha \frac{\partial \operatorname{cost}\left(w_{0}+w_{1} x_{1}\right)}{\partial w_{1}}
\end{array}
$$</p><blockquote><p>理解：α为学习速率，需要手动指定(超参数)，α旁边的整体表示方向沿着这个函数下降的方向找，最后就能找到山谷的最低点，然后更新W值
使用：面对训练数据规模十分庞大的任务，能够找到较好的结果</p></blockquote><p align=middle><img src=https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/20231126214530.png width=20% height=20% alt=效果图></p><p align=middle><img src=https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/20231126214544.png width=20% height=20% alt=效果图></p><h5 id=sklearn实现梯度下降>sklearn实现梯度下降</h5><ul><li><code>sklearn.linear_model.SGDRegressor(loss="squared_loss",fit_intercept=True,learning_rate ='invscaling',eta0=0.01)</code> :实现了随机梯度下降学习，它支持不同的loss函数和正则化惩罚项来拟合线性回归模型。<ul><li>loss:损失类型<ul><li>loss=”squared_loss&rsquo;”:普通最小二乘法</li></ul></li><li>fit_intercept:是否计算偏置</li><li><code>learning_rate string,optional</code>:学习率填充<ul><li><code>'constant':eta =etao</code><ul><li>对于一个常数值的学习率来说，可以使用learning_rate=&lsquo;constant&rsquo;,并使用eta0来指定学习率。</li></ul></li><li>&lsquo;optimal&rsquo;:eta 1.0/(alpha *(t to))[default]</li><li>&lsquo;invscaling&rsquo;:eta etao/pow(t,power_t)<ul><li>power_.t=0.25:存在父类当中</li></ul></li></ul></li><li>SGDRegressor.coef_:回归系数</li><li>SGDRegressor.intercept_:偏置</li></ul></li></ul><h3 id=模型评估>模型评估</h3><p>均方误差(Mean Squared Error))MSE)评价机制：</p><p>$$
MSE=\frac{1}{m} \sum_{i=1}^{m}\left(y^{i}-\bar{y}\right)^{2}
$$</p><blockquote><p>注：y为预测值，y为真实值</p></blockquote><ul><li>sklearn.metrics.mean_squared_error(y_true,y_pred)<ul><li>均方误差回归损失</li><li>y_true:真实值</li><li>y_pred:预测值</li><li>return:浮点数结果</li></ul></li></ul><h3 id=举例>举例</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.datasets</span> <span class=kn>import</span> <span class=n>load_boston</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>StandardScaler</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>LinearRegression</span><span class=p>,</span> <span class=n>SGDRegressor</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>mean_squared_error</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>linear1</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    正规方程的优化方法对波士顿房价进行预测
</span></span></span><span class=line><span class=cl><span class=s2>    :return:
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 1）获取数据</span>
</span></span><span class=line><span class=cl>    <span class=n>boston</span> <span class=o>=</span> <span class=n>load_boston</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 2）划分数据集</span>
</span></span><span class=line><span class=cl>    <span class=n>x_train</span><span class=p>,</span> <span class=n>x_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>boston</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>boston</span><span class=o>.</span><span class=n>target</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>22</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 3）标准化</span>
</span></span><span class=line><span class=cl>    <span class=n>transfer</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>x_train</span> <span class=o>=</span> <span class=n>transfer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>x_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>x_test</span> <span class=o>=</span> <span class=n>transfer</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>x_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 4）预估器</span>
</span></span><span class=line><span class=cl>    <span class=n>estimator</span> <span class=o>=</span> <span class=n>LinearRegression</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>estimator</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>x_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 5）得出模型</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;正规方程-权重系数为：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>estimator</span><span class=o>.</span><span class=n>coef_</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;正规方程-偏置为：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>estimator</span><span class=o>.</span><span class=n>intercept_</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 6）模型评估</span>
</span></span><span class=line><span class=cl>    <span class=n>y_predict</span> <span class=o>=</span> <span class=n>estimator</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>x_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;预测房价：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>y_predict</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>error</span> <span class=o>=</span> <span class=n>mean_squared_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_predict</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;正规方程-均方误差为：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>error</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>linear2</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    梯度下降的优化方法对波士顿房价进行预测
</span></span></span><span class=line><span class=cl><span class=s2>    :return:
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 1）获取数据</span>
</span></span><span class=line><span class=cl>    <span class=n>boston</span> <span class=o>=</span> <span class=n>load_boston</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;特征数量：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>boston</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 2）划分数据集</span>
</span></span><span class=line><span class=cl>    <span class=n>x_train</span><span class=p>,</span> <span class=n>x_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>boston</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>boston</span><span class=o>.</span><span class=n>target</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>22</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 3）标准化</span>
</span></span><span class=line><span class=cl>    <span class=n>transfer</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>x_train</span> <span class=o>=</span> <span class=n>transfer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>x_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>x_test</span> <span class=o>=</span> <span class=n>transfer</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>x_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 4）预估器</span>
</span></span><span class=line><span class=cl>    <span class=n>estimator</span> <span class=o>=</span> <span class=n>SGDRegressor</span><span class=p>(</span><span class=n>learning_rate</span><span class=o>=</span><span class=s2>&#34;constant&#34;</span><span class=p>,</span> <span class=n>eta0</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span> <span class=n>max_iter</span><span class=o>=</span><span class=mi>10000</span><span class=p>,</span> <span class=n>penalty</span><span class=o>=</span><span class=s2>&#34;l1&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>estimator</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>x_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 5）得出模型</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;梯度下降-权重系数为：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>estimator</span><span class=o>.</span><span class=n>coef_</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;梯度下降-偏置为：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>estimator</span><span class=o>.</span><span class=n>intercept_</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 6）模型评估</span>
</span></span><span class=line><span class=cl>    <span class=n>y_predict</span> <span class=o>=</span> <span class=n>estimator</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>x_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;预测房价：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>y_predict</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>error</span> <span class=o>=</span> <span class=n>mean_squared_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_predict</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;梯度下降-均方误差为：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>error</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>linear1</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>linear2</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>两种方法的对比</p><table><thead><tr><th>梯度下降</th><th>正规方程</th></tr></thead><tbody><tr><td>需要选择学习率</td><td>不需要</td></tr><tr><td>需要迭代求解</td><td>一次运算得出</td></tr><tr><td>特征数量较大可以使用</td><td>需要计算方程，时间复杂度高$O(n^3)$</td></tr></tbody></table><ul><li>选择：<ul><li>小规模数据：<ul><li>LinearRegression(不能解决拟合问题)</li><li>岭回归</li></ul></li><li>大规模数据：<ul><li>SGDRegressor</li></ul></li></ul></li></ul><h2 id=欠拟合与过拟合>欠拟合与过拟合</h2><ul><li>过拟合：一个假设在训练数据上能够获得比其他假设更好的拟合，但是在测试数据集上却不能很好地拟合数据，此时认为这个假设出现了过拟合的现象。（模型过于复杂)</li><li>欠拟合：一个假设在训练数据上不能获得更好的拟合，并且在测试数据集上也不能很好地拟合数据，此时认为这个假设出现了欠拟合的现象。（模型过于简单）</li></ul><p align=middle><img src=https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/20231126215838.png width=20% height=20% alt=效果图></p><hr><ul><li>欠拟合原因以及解决办法<ul><li>原因：学习到数据的特征过少</li><li>解决办法：增加数据的特征数量</li></ul></li><li>过拟合原因以及解决办法<ul><li>原因：原始特征过多，存在一些嘈杂特征，模型过于复杂是因为模型尝试去兼顾各个测试数据点</li><li>解决办法：正则化</li></ul></li></ul><p>在这里针对回归，我们选择了正则化。但是对于其他机器学习算法如分类算法来说也会出现这样的问题，除了一些算法本身作用之外（决策树、神经网络），我们更多的也是去自己做特征选择，包括之前说的删除、合并一些特征</p><p align=middle><img src=https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/20231126215928.png width=20% height=20% alt=效果图></p><blockquote><p>在学习的时候，数据提供的特征有些影响模型复杂度或者这个特征的数据点异常较多，所以算法在学习的时候尽量减少这个特征的影响（甚至刚除某个特征的影响)，这就是正则化</p><blockquote><p>注：调整时候，算法并不知道某个特征影响，而是去调超参数得出优化的结果</p></blockquote></blockquote><h3 id=正则化类别>正则化类别</h3><p>L2正则化</p><ul><li>作用：可以使得其中一些W的都很小，都接近于0，削弱某个特征的影响</li><li>优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象</li><li>Ridge回归</li><li>加入L2正则化后的损失函数：</li></ul><p>$$
J(w)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{w}\left(x_{i}\right)-y_{i}\right)^{2}+\lambda \sum_{j=1}^{n} w_{j}^{2}
$$</p><blockquote><p>注：m为样本数，n为特征数
$h_w(x_i)$ 为预测值，$y_i$ 为真实值，L1正则化就是把 $w_{j}^{2}$ 改为 $|w_j|$</p></blockquote><h2 id=线性回归的改进-岭回归>线性回归的改进-岭回归</h2><blockquote><p>带有L2正则化的线性回归(岭回归)</p></blockquote><p>岭回归，其实也是一种线性回归。只不过在算法建立回归方程时候，加上正则化的限制，从而达到解决过拟合的效果</p><ul><li><code>sklearn.linear_model.Ridge(alpha=1.0,fit_intercept=True,solver="auto",normalize=False)</code>:具有2正则化的线性回归<ul><li>alpha:正则化力度，也叫入SAG<ul><li>λ取值：0~11~10</li></ul></li><li>solver:会根据数据自动选择优化方法<ul><li>sag如果数据集、特征都比较大，选择该随机梯度下降优化</li></ul></li><li>normalize:数据是否进行标准化<ul><li>normalize=False:可以在fit之前调用preprocessing.StandardScaler标准化数据</li></ul></li><li>Ridge.coef:回归权重</li><li>Ridge.intercept:回归偏置</li></ul></li></ul><p>Ridge方法相当于<code>SGDRegressor(penalty:='I2',loss="squared_loss")</code>,只不过SGDRegressor实现了一个普通的随机梯度下降学习，推荐使用Ridge(实现了SAG)</p><ul><li><code>sklearn.linear_model.RidgeCV(BaseRidgeCV,RegressorMixin)</code>:具有2正则化的线性回归，可以进行交叉验证<ul><li>coef:回归系数</li></ul></li></ul><p align=middle><img src=https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/20231126221132.png width=20% height=20% alt=效果图></p><p>正则化力度越大，权重系数会越小
正则化力度越小，权重系数会越大</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.datasets</span> <span class=kn>import</span> <span class=n>load_boston</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>StandardScaler</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>LinearRegression</span><span class=p>,</span> <span class=n>SGDRegressor</span><span class=p>,</span><span class=n>Ridge</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>mean_squared_error</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>linear3</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    岭回归对波士顿房价进行预测
</span></span></span><span class=line><span class=cl><span class=s2>    :return:
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 1）获取数据</span>
</span></span><span class=line><span class=cl>    <span class=n>boston</span> <span class=o>=</span> <span class=n>load_boston</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;特征数量：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>boston</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 2）划分数据集</span>
</span></span><span class=line><span class=cl>    <span class=n>x_train</span><span class=p>,</span> <span class=n>x_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>boston</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>boston</span><span class=o>.</span><span class=n>target</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>22</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 3）标准化</span>
</span></span><span class=line><span class=cl>    <span class=n>transfer</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>x_train</span> <span class=o>=</span> <span class=n>transfer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>x_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>x_test</span> <span class=o>=</span> <span class=n>transfer</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>x_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>#4）预估器</span>
</span></span><span class=line><span class=cl>    <span class=n>estimator</span> <span class=o>=</span> <span class=n>Ridge</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> <span class=n>max_iter</span><span class=o>=</span><span class=mi>10000</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>estimator</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>x_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 5）得出模型</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;岭回归-权重系数为：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>estimator</span><span class=o>.</span><span class=n>coef_</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;岭回归-偏置为：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>estimator</span><span class=o>.</span><span class=n>intercept_</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 6）模型评估</span>
</span></span><span class=line><span class=cl>    <span class=n>y_predict</span> <span class=o>=</span> <span class=n>estimator</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>x_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;预测房价：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>y_predict</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>error</span> <span class=o>=</span> <span class=n>mean_squared_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_predict</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;岭回归-均方误差为：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>error</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>linear3</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><hr><h2 id=逻辑回归>逻辑回归</h2><p>逻辑回归(Logistic Regression)是机器学习中的一种分类模型，逻辑回归是一种<strong>分类算法</strong>，虽然名字中带有回归，但是它与回归之间有一定的联系。由于算法的简单和高效，在实际中应用非常广泛。</p><h3 id=原理>原理</h3><h4 id=输入>输入</h4><p>$$
h(w)=w_{1} x_{1}+w_{2} x_{2}+w_{3} x_{3} \ldots+\mathrm{b}
$$</p><blockquote><p>逻辑回归的输入就是一个线性回归的结果。</p></blockquote><h4 id=激活函数>激活函数</h4><p>sigmoid函数</p><p>$$
g\left(\theta^{T} x\right)=\frac{1}{1+e^{-\theta^{T} x}}
$$</p><ul><li>分析<ul><li>回归的结果输入到sigmoid函数当中</li><li>输出结果：[0,1]区间中的一个概率值，默认为0.5为阈值</li></ul></li></ul><p align=middle><img src=https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/20231126221844.png width=20% height=20% alt=效果图></p><blockquote><p>逻辑回归最终的分类是通过属于某个类别的概率值来判断是否属于某个类别，并且这个类别默认标记为1（正例，另外的一个类别会标记为0（反例）。（方便损失计算)</p></blockquote><p>输出结果解释（重要）：假设有两个类别A,B,并且假设我们的概率值为属于A(1)这个类别的概率值。现在有一个样本的输入到逻辑回归输出结果0.6，那么这个概率值超过0.5，意味着我们训练或者预测的结果就是A(1)类别。那么反之，如果得出结果为0.3那么，训练或者预测结果就为B(O)类别。</p><p>所以接下来我们回忆之前的线性回归预测结果我们用均方误差衡量，那如果对于逻辑回归，我们预测的结果不对该怎么去衡量这个损失呢？我们来看这样一张图</p><p align=middle><img src=https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/20231126222041.png width=20% height=20% alt=效果图></p><blockquote><p>假设得出概率值是属于A的概率值</p></blockquote><p>那么如何去衡量逻辑回归的预测结果与真实结果解差异呢？</p><h4 id=损失函数-1>损失函数</h4><p>1损失
逻辑回归的损失，称之为对数似然损失，公式如下：
·分开类别：</p><p>$$
\operatorname{cost}\left(h_{\theta}(x), y\right)=\left{\begin{array}{ll}
-\log \left(h_{\theta}(x)\right) & \text { if } \mathrm{y}=1 \
-\log \left(1-h_{\theta}(x)\right) & \text { if } \mathrm{y}=0
\end{array}\right.
$$</p><p>当y=1时：</p><p align=middle><img src=https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/20231126222403.png width=20% height=20% alt=效果图></p><p>同理，我们也可以画出当y=0时，函数C()的图像：</p><p align=middle><img src=https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/20231126222439.png width=20% height=20% alt=效果图></p><p>综合完整损失函数</p><p>$$
\operatorname{cost}\left(h_{\theta}(x), y\right)=\sum_{i=1}^{m}-y_{i} \log \left(h_{\theta}(x)\right)-\left(1-y_{i}\right) \log \left(1-h_{\theta}(x)\right)
$$</p><p align=middle><img src=https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/20231126222615.png width=20% height=20% alt=效果图></p><h4 id=优化损失>优化损失</h4><p>同样使用梯度下降优化算法，去减少损失函数的值。这样去更新逻辑回归前面对应算
法的权重参数，提升原本属于1类别的概率，降低原本是0类别的概率</p><h3 id=逻辑回归api>逻辑回归API</h3><ul><li>sklearn.linear_model.LogisticRegression(solver=&lsquo;liblinear&rsquo;,penalty=&lsquo;12&rsquo;,C=1.0)<ul><li>solver:优化求解方式(默开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数)<ul><li>Sag:根据数据集自动选择，随机平均梯度下降</li></ul></li><li>penalty:正则化的种类</li><li>C:正则化力度</li></ul></li></ul><blockquote><p>默认将类别数量少的当做正例</p></blockquote><p>LogisticRegression方法相当于<code>SGDClassifier(loss="log",penalty="")</code>,SGDClassifier实现了一个普通的随机梯度下降学习，也支持平均随机梯度下降法(ASGD)，可以通过设置average:=True。而使用LogisticRegression(实现了SAG)</p><h2 id=模型评估二分类>模型评估(二分类)</h2><h3 id=精确率与召回率>精确率与召回率</h3><ol><li>混淆矩阵</li></ol><blockquote><p>在分类任务下，预测结果(Predicted Condition)与正确标记(True Condition)之间存在四种不同的组合，构成混淆矩阵（适用于多分类）</p></blockquote><table><thead><tr><th></th><th>预测.正例</th><th>预测.假例</th></tr></thead><tbody><tr><td>真实.正例</td><td>真正例TP</td><td>伪反例FN</td></tr><tr><td>真实.假例</td><td>伪正例FP</td><td>真反例TN</td></tr></tbody></table><ol start=2><li>精确率(Precision)与召回率(Recall)</li></ol><ul><li>精确率: 预测结果为正例样本中真实为正例的比例</li></ul><table><thead><tr><th></th><th>$\color{red}{预测.正例}$</th><th>预测.假例</th></tr></thead><tbody><tr><td>$\color{red}{真实.正例}$</td><td>$\color{red}{真正例TP}$</td><td>伪反例FN</td></tr><tr><td>真实.假例</td><td>$\color{red}{伪正例FP}$</td><td>真反例TN</td></tr></tbody></table><ul><li>召回率：真实为正例的样本中预测结果为正例的比例 (查得全，对<strong>正样本的区分能力</strong>)</li></ul><table><thead><tr><th></th><th>$\color{red}{预测.正例}$</th><th>预测.假例</th></tr></thead><tbody><tr><td>$\color{red}{真实.正例}$</td><td>$\color{red}{真正例TP}$</td><td>$\color{red}{伪反例FN}$</td></tr><tr><td>真实.假例</td><td>伪正例FP</td><td>真反例TN</td></tr></tbody></table><ul><li>还有其他的评估标准, F1-score, 反映了模型的稳健型</li></ul><p>$$
F1=\frac{2 T P}{2 T P+F N+F P}=\frac{2 \cdot \text { Precision } \cdot \text { Recall }}{\text { Precision }+ \text { Recall }}
$$</p><h3 id=分类评估报告api>分类评估报告API</h3><ul><li><code>sklearn.metrics.classification_report(y_true,y_pred,labels,target_names=None)</code><ul><li>y_true:真实目标值</li><li>y_pred:估计器预测目标值</li><li>labels:指定类别对应的数字</li><li>target_names:目标类别名称</li><li>return:每个类别精确率与召回率</li></ul></li></ul><p>假设这样一个情况，如果99个样本癌症，1个样本非癌症，不管怎样我全都预测正例(默认癌症为正例)，准确率就为99%但是这样效果并不好，这就是样本不均衡下的评估问题</p><h3 id=如何衡量样本不均衡下的评估>如何衡量样本不均衡下的评估？</h3><h4 id=roc曲线与auc指标>ROC曲线与AUC指标</h4><blockquote><p>知道TPR与FPR</p><ul><li>$ TPR=\frac{TP}{TP+FN} $: 所有真实类别为1的样本中，预测类别为1的比例</li><li>$ FPR=\frac{FP}{FP+TN} $: 所有真实类别为0的样本中，预测类别为1的比例</li></ul></blockquote><ol start=2><li>ROC曲线</li></ol><p>ROC曲线的横轴就是FPRate,纵轴就是TPRate,当二者相等时，表示的意义则是：对于不论真实类别是1还是0的样本，分类器预测为1的概率是相等的，此时AUC为0.5</p><p align=middle><img src=https://raw.githubusercontent.com/zput/myPicLib/master/zput.github.io/20231126224250.png width=20% height=20% alt=效果图></p><ol start=3><li>AUC指标</li></ol><ul><li>AUC的概率意义是随机取一对正负样本，正样本得分大于负样本的概率</li><li>AUC的最小值为0.5，最大值为1，取值越高越好</li><li>AUC=1,完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。</li><li>0.5&lt;AUC&lt;1,优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。</li></ul><blockquote><p>最终AUC的范围在[0.5,1]之间，并且越接近1越好</p></blockquote><ol start=4><li>AUC计算API</li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>roc_auc_score</span>
</span></span><span class=line><span class=cl><span class=n>sklearn</span><span class=o>.</span><span class=n>metrics</span><span class=o>.</span><span class=n>roc_auc_score</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span><span class=n>y_score</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1>#计算ROC曲线面积，即AUC值</span>
</span></span><span class=line><span class=cl><span class=c1># y_tue:每个样本的真实类别，必须为0（反例，1（正例标记</span>
</span></span><span class=line><span class=cl><span class=c1># y_score:预测得分，可以是正类的估计概率、置信值或者分类器方法的返回值</span>
</span></span></code></pre></td></tr></table></div></div></div></div><div class=post-copyright><p class=copyright-item><span class=item-title>文章作者</span>
<span class=item-content>zput</span></p><p class=copyright-item><span class=item-title>上次更新</span>
<span class=item-content>2023-11-09</span></p></div><footer class=post-footer><nav class=post-nav><a class=prev href=/post/_posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/><i class="iconfont icon-left"></i>
<span class="prev-text nav-default">无监督学习</span>
<span class="prev-text nav-mobile">上一篇</span>
</a><a class=next href=/post/_posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/sklearn/><span class="next-text nav-default">sklearn</span>
<span class="next-text nav-mobile">下一篇</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div><script src=https://giscus.app/client.js data-repo=zput/utterances-comments data-repo-id=R_kgDOHhATGQ data-category=Announcements data-category-id=DIC_kwDOHhATGc4CPxca data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=1 data-input-position=top data-theme=light data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:wkzxc@sina.com class="iconfont icon-email" title=email></a><a href=https://github.com/zput class="iconfont icon-github" title=github></a><a href=http://zput.github.io/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=copyright-year>&copy;
2017 -
2023<span class=heart><i class="iconfont icon-heart"></i></span><span>zput</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script><script type=text/javascript src=/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js></script><script type=text/javascript>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script></body></html>