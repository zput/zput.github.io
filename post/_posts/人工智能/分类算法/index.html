<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>k近邻算法 - Zput's blog</title>
<meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=Cache-Control content="no-transform"><meta http-equiv=Cache-Control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="zput"><meta name=description content="kNN kNN（k-Nearest Neighbors）算法是一种简单但常用的分类和回归算法。它的核心思想是基于相似性来做预测。具体来说，对于一个测"><meta name=keywords content="Hugo,theme,even"><meta name=generator content="Hugo 0.120.4 with theme even"><link rel=canonical href=http://zput.github.io/post/_posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link href=/sass/main.min.e291ec317bf4a8f48812dc8bcd749ad9270976917bf2027d01a4415caecb80a5.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="k近邻算法"><meta property="og:description" content="kNN kNN（k-Nearest Neighbors）算法是一种简单但常用的分类和回归算法。它的核心思想是基于相似性来做预测。具体来说，对于一个测"><meta property="og:type" content="article"><meta property="og:url" content="http://zput.github.io/post/_posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"><meta property="article:section" content="post"><meta property="article:published_time" content="2023-11-02T19:00:00+00:00"><meta property="article:modified_time" content="2023-11-02T19:00:00+00:00"><meta itemprop=name content="k近邻算法"><meta itemprop=description content="kNN kNN（k-Nearest Neighbors）算法是一种简单但常用的分类和回归算法。它的核心思想是基于相似性来做预测。具体来说，对于一个测"><meta itemprop=datePublished content="2023-11-02T19:00:00+00:00"><meta itemprop=dateModified content="2023-11-02T19:00:00+00:00"><meta itemprop=wordCount content="691"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="k近邻算法"><meta name=twitter:description content="kNN kNN（k-Nearest Neighbors）算法是一种简单但常用的分类和回归算法。它的核心思想是基于相似性来做预测。具体来说，对于一个测"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>Zput</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/go-goroutine><li class=mobile-menu-item>go源码专栏</li></a><a href=/post/mrmk/><li class=mobile-menu-item>中间件专栏</li></a><a href=/post/cloud_native/><li class=mobile-menu-item>云原生专栏</li></a><a href=/post/postgraduate/><li class=mobile-menu-item>考研专栏</li></a><a href=/post/><li class=mobile-menu-item>时间线</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>Zput</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/go-goroutine>go源码专栏</a></li><li class=menu-item><a class=menu-item-link href=/post/mrmk/>中间件专栏</a></li><li class=menu-item><a class=menu-item-link href=/post/cloud_native/>云原生专栏</a></li><li class=menu-item><a class=menu-item-link href=/post/postgraduate/>考研专栏</a></li><li class=menu-item><a class=menu-item-link href=/post/>时间线</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>k近邻算法</h1><div class=post-meta><span class=post-time>2023-11-02</span><div class=post-category><a href=/categories/applicationandsoftware/>applicationAndSoftware</a></div></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class="post-toc-content always-active"><nav id=TableOfContents><ul><li><ul><li><a href=#knn>kNN</a><ul><li><a href=#原理>原理</a></li><li><a href=#举例>举例</a></li><li><a href=#优缺点>优缺点</a></li></ul></li></ul></li></ul></nav></div></div><div class=article-container><div class=post-content><h2 id=knn>kNN</h2><p>kNN（k-Nearest Neighbors）算法是一种简单但常用的分类和回归算法。它的核心思想是基于相似性来做预测。具体来说，对于一个测试样本，在训练数据中找到与其距离最近的 k 个训练样本，然后根据这 k 个样本的标签进行预测。</p><h3 id=原理>原理</h3><p>对于给定的一个测试样本x，kNN 算法的分类过程如下：</p><ul><li>计算测试样本 x 与每个训练样本之间的距离；</li><li>选取距离最近的 k 个训练样本；</li><li>根据这 k 个样本的标签，通过投票的方式确定测试样本的类别。</li></ul><p>对于回归问题而言，kNN 算法的过程也是类似的，只不过最后的预测结果是这k个样本的平均值。</p><p>在实现时，我们需要选择一个合适的距离度量方法和一个合适的 k 值。
通常，欧式距离是最常用的距离度量方法。而 k 的取值则需要通过交叉验证等方法来确定。</p><h3 id=举例>举例</h3><p>假设我们有如下的数据集，其中每个样本包含两个特征 $x_1$ 和 $x_2$，以及它们对应的类别标签：</p><table><thead><tr><th>x1</th><th>x2</th><th>标签</th></tr></thead><tbody><tr><td>1</td><td>1</td><td>A</td></tr><tr><td>1</td><td>2</td><td>A</td></tr><tr><td>2</td><td>1</td><td>A</td></tr><tr><td>4</td><td>3</td><td>B</td></tr><tr><td>5</td><td>2</td><td>B</td></tr><tr><td>6</td><td>4</td><td>B</td></tr></tbody></table><p>现在我们要对以下测试样本进行分类：</p><table><thead><tr><th>$x_1$</th><th>$x_2$</th></tr></thead><tbody><tr><td>3</td><td>2</td></tr></tbody></table><p>我们使用 kNN 算法进行分类，假设 k=3。首先计算测试样本与训练集中每个样本之间的距离：</p><table><thead><tr><th>距离</th><th>标签</th></tr></thead><tbody><tr><td>$\sqrt{(3-1)^2+(2-1)^2}=\sqrt{5}$</td><td>A</td></tr><tr><td>$\sqrt{(3-1)^2+(2-2)^2}=2$</td><td>A</td></tr><tr><td>$\sqrt{(3-2)^2+(2-1)^2=\sqrt{2}}$</td><td>A</td></tr><tr><td>$\sqrt{(3-4)^2+(2-3)^2}=\sqrt{2}$</td><td>B</td></tr><tr><td>$\sqrt{(3-5)^2+(2-2)^2}=2$</td><td>B</td></tr><tr><td>$\sqrt{(3-6)^2+(2-4)^2}=\sqrt{13}$</td><td>B</td></tr></tbody></table><p>可以发现，距离最近的三个样本分别是 $(1,2)$、$(2,1)$ 和 $(1,1)$，它们的类别分别是 A、A 和 A。因此，这三个样本的投票结果是 A，所以测试样本 $(3,2)$ 被归类为 A 类。</p><h3 id=优缺点>优缺点</h3><ul><li><p>kNN 算法的优点包括：</p><ul><li>简单、易于理解和实现；</li><li>对于多分类问题效果很好；</li><li>适用于大规模数据集，需要的存储空间少。</li></ul></li><li><p>kNN 算法的缺点包括：</p><ul><li>需要事先确定k的取值；</li><li>对于高维数据或者特征空间非常稀疏的数据，效果较差；</li><li>需要计算测试样本与所有训练样本之间的距离，计算量较大；</li><li>对于不平衡数据集的处理较为困难。</li></ul></li></ul></div></div><div class=post-copyright><p class=copyright-item><span class=item-title>文章作者</span>
<span class=item-content>zput</span></p><p class=copyright-item><span class=item-title>上次更新</span>
<span class=item-content>2023-11-02</span></p></div><footer class=post-footer><nav class=post-nav><a class=prev href=/post/_posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/sklearn/><i class="iconfont icon-left"></i>
<span class="prev-text nav-default">sklearn</span>
<span class="prev-text nav-mobile">上一篇</span>
</a><a class=next href=/post/_posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4/><span class="next-text nav-default">特征降维</span>
<span class="next-text nav-mobile">下一篇</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div><script src=https://giscus.app/client.js data-repo=zput/utterances-comments data-repo-id=R_kgDOHhATGQ data-category=Announcements data-category-id=DIC_kwDOHhATGc4CPxca data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=1 data-input-position=top data-theme=light data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:wkzxc@sina.com class="iconfont icon-email" title=email></a><a href=https://github.com/zput class="iconfont icon-github" title=github></a><a href=http://zput.github.io/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=copyright-year>&copy;
2017 -
2023<span class=heart><i class="iconfont icon-heart"></i></span><span>zput</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script><script type=text/javascript src=/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js></script><script type=text/javascript>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script></body></html>